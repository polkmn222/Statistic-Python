{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stratified K fold :: 층화 K fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Stratified K 폴드는 불균형한(imbalanced) 분포도를 가진 레이블(결정 클래스) 데이터 집합을 위한 K폴드 방식입니다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "가령 대출 사기 데이터를 예측한다고 가정해보자. 이 데이터 셋은 1억 건이고, 수십 개의 피처와 대출사기 여부를 뜻하는 레이블(대출사기:1, 정상대출:0)로 구성돼 있다. 그런데 대부분의 데이터는 정상 대출일 것이다.그리고 대출 사기가 약 1000건이 있다고 한다면 전체의 0.0001%의 아주 작은 확률로 대출 사기 레이블이 존재한다. 이렇게 작은 비율로 1 레이블 값이 있다면 K 폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 레이블 값인 0과 1의 비율을 제대로 반영하지 못하는 경우가 쉽게 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "즉, 레이블 값으로 1이 특정 개별 반복별 학습/테스트 데이터 세트에는 상대적으로 많이 들어 있고, 다른 반복 학습/테스트 데이터 세트에는 그렇지 못한 결과가 발생한다. 대출 사기 레이블이 1인 레코드는 비록 건수는 작지만 알고리즘이 대출 사기를 예측하기 위한 중요한 피처 값을 가지고 있기 때문에 매우 중요한 데이터 세트이다.\n",
    "따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df.head()\n",
    "\n",
    "# 각 값의 구성확인\n",
    "iris_df['target'].value_counts()\n",
    "\n",
    "## setosa, virsicolor, virginica 각 품종이 50개씩 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.target[1]\n",
    "iris_df.target.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## CV:1\n",
      "y_train 데이터 분포:\n",
      " 1    50\n",
      "2    50\n",
      "Name: target, dtype: int64\n",
      "y_val 데이터 분포:\n",
      " 0    50\n",
      "Name: target, dtype: int64\n",
      "## CV:2\n",
      "y_train 데이터 분포:\n",
      " 0    50\n",
      "2    50\n",
      "Name: target, dtype: int64\n",
      "y_val 데이터 분포:\n",
      " 1    50\n",
      "Name: target, dtype: int64\n",
      "## CV:3\n",
      "y_train 데이터 분포:\n",
      " 0    50\n",
      "1    50\n",
      "Name: target, dtype: int64\n",
      "y_val 데이터 분포:\n",
      " 2    50\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# KFold 수행을 해보자.\n",
    "kfold = KFold(n_splits=3)\n",
    "n_iter = 0\n",
    "for train_index, val_index in kfold.split(iris_df):\n",
    "    n_iter = n_iter + 1  # n_iter += 1\n",
    "    y_train = iris_df['target'][train_index]\n",
    "    y_val = iris_df['target'][val_index]\n",
    "    print('## CV:{0}'.format(n_iter))\n",
    "    print('y_train 데이터 분포:\\n', y_train.value_counts())\n",
    "    print('y_val 데이터 분포:\\n', y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## CV:1\n",
      "y_train 데이터 분포:\n",
      " 2    34\n",
      "0    33\n",
      "1    33\n",
      "Name: target, dtype: int64\n",
      "y_val 데이터 분포:\n",
      " 0    17\n",
      "1    17\n",
      "2    16\n",
      "Name: target, dtype: int64\n",
      "## CV:2\n",
      "y_train 데이터 분포:\n",
      " 1    34\n",
      "0    33\n",
      "2    33\n",
      "Name: target, dtype: int64\n",
      "y_val 데이터 분포:\n",
      " 0    17\n",
      "2    17\n",
      "1    16\n",
      "Name: target, dtype: int64\n",
      "## CV:3\n",
      "y_train 데이터 분포:\n",
      " 0    34\n",
      "1    33\n",
      "2    33\n",
      "Name: target, dtype: int64\n",
      "y_val 데이터 분포:\n",
      " 1    17\n",
      "2    17\n",
      "0    16\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### 위의 방법으로 데이터가 균일하지 않으므로\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "n_iter = 0\n",
    "for train_index, val_index in skf.split(iris_df, iris_df['target']):\n",
    "    n_iter = n_iter + 1  # n_iter += 1\n",
    "    y_train = iris_df['target'][train_index]\n",
    "    y_val = iris_df['target'][val_index]\n",
    "    print('## CV:{0}'.format(n_iter))\n",
    "    print('y_train 데이터 분포:\\n', y_train.value_counts())\n",
    "    print('y_val 데이터 분포:\\n', y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 교차 검증을 보다 간편하게-cross_val_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "사이킷런은 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API를 제공한다. 대표적인 것이 cross_val_score()이다. KFold로 데이터를 학습하고 예측하는 코드를 보면 먼저 (1)폴드 세트를 설정하고 (2)for 루프에서 반복으로 학습 및 테스트 데이터의 인덱스를 추출한 뒤 (3)반복적으로 학습과 예측을 수행하고 예측 성능을 반환했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "cross_val_score()는 이런 일련의 과정을 한꺼번에 수행해주는 API이다. 다음은 cross_val_score()API의 선언 형태이다. cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’). 이 중 estimator, X, y, scoring, cv가 주요 파라미터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도: [0.98 0.94 0.98]\n",
      "평균 검증별 정확도: 0.9667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "dt_clf=DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "data= iris_data.data\n",
    "label=iris_data.target\n",
    "\n",
    "#성능 지표는 정확도(accuracy), 교차 검증 세트는 3개\n",
    "scores= cross_val_score(dt_clf,data,label,scoring='accuracy', cv=3)\n",
    "print('교차 검증별 정확도:', np.round(scores, 4))\n",
    "print('평균 검증별 정확도:', np.round(np.mean(scores),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# end of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
